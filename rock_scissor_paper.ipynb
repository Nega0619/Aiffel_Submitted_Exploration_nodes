{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f0dc106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success imort PIL lib\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "print(\"Success imort PIL lib\")\n",
    "\n",
    "def resize_images(img_path):\n",
    "    images=glob.glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "07ad1ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9d4debbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300  images to be resized.\n",
      "300  images resized.\n",
      "가위 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "    print(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "    \n",
    "    print(len(images), \" images resized.\")\n",
    "    \n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"가위 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dbf82255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300  images to be resized.\n",
      "300  images resized.\n",
      "rock 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/rock\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"rock 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "27e1dd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300  images to be resized.\n",
      "300  images resized.\n",
      "paper 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/paper\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"paper 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6525c819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 900 입니다.\n",
      "x_train shape: (900, 28, 28, 3)\n",
      "0.9647058823529412 0.0\n",
      "y_train shape: (900,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(img_path, number_of_data=900, rock_path='/rock/*.jpg', scissor_path='/scissor/*.jpg', paper_path='/paper/*.jpg'):  # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+scissor_path):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+rock_path):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+paper_path):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_train, y_train)=load_data(image_dir_path)\n",
    "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(np.max(x_train_norm), np.min(x_train_norm))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ae93c4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVmklEQVR4nO2dXYxc5XnH//8zH/tp7LWxF9uhCY1oVVSppFqhSkEtVZSUcAO5QeEiohKqcxGkRMpFEb0Il6gqiSK1iuQUFKdKiSIlCC5QG4oiodxELOCC+WhNqVHsLraxDV57P2Zn5unFDNEG9n2eYc58Ke//J6129zzznvPO2fnPmT3/93kemhmEEL/7FOOegBBiNEjsQmSCxC5EJkjsQmSCxC5EJlRHebCp+pTNzc4l44x2wPQjnFBPMNiBHw4OXi4cz63w4uVOTHxeowek3Z6CwbUmOHjkJHlR/5zF57y8h+W8lvsfiksXL+HqlSs7PqKU2EneDuC7ACoA/tnMHvYePzc7h7/6i897+3OPVxTpF0el6o+tVCpuvFr1T0Wl4uy/4r9oo2M7TwsAUKvV/Hg9vf/o2BHeOe9l/2ynZTE1NRUc3P+bNNstN95y/mS1qXpwaD/eDuRuJd5MyOCcOmP/8ZFHkrG+P8azM6N/AvBFADcBuIfkTf3uTwgxXMr8z34LgDfN7C0zawD4MYA7BzMtIcSgKSP2wwB+ve33091tvwXJIySXSS5vNjZLHE4IUYah3403s6NmtmRmS1P14H80IcTQKCP2MwCu3/b7J7rbhBATSBmxPw/gRpI3kKwD+DKApwYzLSHEoOnbejOzJsn7Afw7OtbbY2b2qjeGBKq19PtLGestts4i681/3/P2H9ks0b6juYXWmxMvAksystbKxutF+rm12213LJyxAFCf9v8tbDt/l9A6C16L9eBvEr0mPMpYb16slM9uZk8DeLrMPoQQo0HLZYXIBIldiEyQ2IXIBIldiEyQ2IXIBIldiEwYaT47Sddzjnx2z4+uVst51ZFPT2f/sRfthlEEc69Ec3NSbCuVcusPovRd0vera5X0eW80Gv6xg9dDlCJrztwbzS13bDu4DobHHpvPnp63ruxCZILELkQmSOxCZILELkQmSOxCZILELkQmjNR6A3yrpgjsCq/Ca5yiGthjTupt59glLMOaf5ojWzCy9trO4b1YZ99Bem5kzQXPvdVKV4CN7M6owmuZcs+R5VgJKttGfxPPqo0YVoqrruxCZILELkQmSOxCZILELkQmSOxCZILELkQmSOxCZMKIfXYDkS4fHLbRLdLOqRcbRBxMz9sCXzSiCHz4et33m1tOSeaorXGUPlstuQagspVOJZ2envb3HfjsURfXtqXPS7i2ITi2xX22g3B/aarRWA9d2YXIBIldiEyQ2IXIBIldiEyQ2IXIBIldiEyQ2IXIhJGXkvbywouwlLRXMjkq5xz5nm7Y9zaDcsorK2fc+L4Di258z8KCG/e8bi+fHIhbF0dtk6M62fPz6ZLL0RqAdhAP20U7ZbSj8t3R2onIZy9XSrqMR58eV0rsJE8BWAXQAtA0s6Uy+xNCDI9BXNn/0szeHcB+hBBDRP+zC5EJZcVuAH5O8gWSR3Z6AMkjJJdJLm9sbJQ8nBCiX8p+jL/VzM6QPADgGZJvmNlz2x9gZkcBHAWA/fv2BXeDhBDDotSV3czOdL+fA/AEgFsGMSkhxODpW+wk50ju+uBnAF8AcGJQExNCDJYyH+MXATzR9fyqAP7VzP7NG0AClbrnlfveplf7vRrUfY/qxkd+sut9Bp7qyZMn3XjTybsGgNlZP+97ZnY+GYvOaVj/PFhDAKc+QXT8lZUVd+z6pt9Wef/iATe+a2FPMhblwm+1/OdVn/ZbNrei9QsOZerGA05vhT7nAzN7C8Cf9DteCDFaZL0JkQkSuxCZILELkQkSuxCZILELkQmjLSVN3wKL0lRd6y0oiczQguo/rdCC0r/vnPUtpn0HrnXjUSroVN157lEb7MIvqRye1yAds9FoJGOnT592x65eXXfjs/Nzbnxhf/q8bq6n5wUAzaZvvc3V0nYnAFir6cb9NFW1bBZClEBiFyITJHYhMkFiFyITJHYhMkFiFyITJHYhMmHELZt9Is/WS8cMMjURdVWuOmWHAd9vjtJI/+gP/8CNXzh/3o0XQbrkzFS6vXCz6fu962urbnz3fr/M9ebmpht/6aWXkrG1tTV37O998gY3fvjwYTceldH2mJ/3ffTotRq1hPYZzjVYV3YhMkFiFyITJHYhMkFiFyITJHYhMkFiFyITJHYhMmHEPjvdVrZhm1sn3z0aG7WDjnzTihP3YgAwO+2Xgr504YIb31r3/eiNK2mvfGZmxh07c80u/9gbV914tEbA47rrrnPji4u+xx+tbzBnfUK9nl6bAMQltiMPn0FL6HGgK7sQmSCxC5EJErsQmSCxC5EJErsQmSCxC5EJErsQmTDyuvEs0ocsGNQoR9q7jMYWQUJ7NfBVvXAlWB6wJ/Cyz7wd5Jyv+jnnG7Pp9sF7gnbPu4L4qVOn3Pj5M2fcuFn6vB86dMgdu3//fjfeClpdO92LQ5+9HVwHo2NH6zZ8+h/rjQyv7CQfI3mO5Ilt2/aSfIbkye73hb5nJ4QYCb18jP8BgNs/tO0BAM+a2Y0Anu3+LoSYYEKxm9lzAC5+aPOdAI51fz4G4K7BTksIMWj6vUG3aGYfNDB7B0ByETPJIySXSS6vr2/0eTghRFlK3423TtfBZMaBmR01syUzW5qZ8W8GCSGGR79iP0vyIAB0v58b3JSEEMOgX7E/BeDe7s/3AnhyMNMRQgyL0Gcn+TiA2wBcS/I0gG8BeBjAT0jeB+BtAHf3cjCCKByfnUHtdi/u7bcT778mPQBUnB7sReCL7l/Y48ZrgVEf5ZTXLd2HvNr2a863Nvxc+Qvv/J8bb1694san96Rz0qPa7NHfJLoHVHHq6Uc+eLvl++jVmv96C1YAjIVQ7GZ2TyL0uQHPRQgxRLRcVohMkNiFyASJXYhMkNiFyASJXYhMGH0pacfC8mKd0U48GhvG+y81HaXH7t3jJwXunptz461N32KqO9m7zU3ftlu96FtnaxfedePXBPbZ/IEDyZjXBhsAGo2GG+8s3kzjlZput31zrBVYb/XpdFpxZ7yftuzjP69+R+rKLkQmSOxCZILELkQmSOxCZILELkQmSOxCZILELkQmjLiUNAHWnLD/3mNF/21wQ5+9HcQdL51BWeH5Wb9t8oF9e914Y833yrG5mQytNdIxAHj/0ntuvEL/uS3Mz7rxfU5b5rDlcuCj12rp11JE5LNHRC2by+5/GOjKLkQmSOxCZILELkQmSOxCZILELkQmSOxCZILELkQmjDif3cei9x7z4sN93yr6TzFGNVgfcGBfuhQ0ALz5xqtu/NLFC8lYPSih3Wr4ufLX7tntxmfrvtft5axHPnpUrhnBc9tw8uErwb6npvx89cbWlhu3YG4+JV5szlhd2YXIBIldiEyQ2IXIBIldiEyQ2IXIBIldiEyQ2IXIhJH67GaAWdp/9GLljx14lyUOHb1jRrveFeSEr6/5bZW9nPQ5p20xAMxO+/GF3de48ei8rq+vJ2NR3fipmaCeflBHYNPJ858L1gdEPvt6UMuf7L/2Qqm68c7Q8MpO8jGS50ie2LbtIZJnSB7vft3R9+yEECOhl4/xPwBw+w7bv2NmN3e/nh7stIQQgyYUu5k9B+DiCOYihBgiZW7Q3U/y5e7H/GQzM5JHSC6TXPb+fxNCDJd+xf49AJ8GcDOAFQCPpB5oZkfNbMnMlmZm/MKLQojh0ZfYzeysmbXMrA3g+wBuGey0hBCDpi+xkzy47dcvATiReqwQYjIIfXaSjwO4DcC1JE8D+BaA20jejI4heArAV3s5WIEWptqr6QeY730a0r7rFnyvuln4vmkzyDn34m36uc17g/7r//vmSTc+V/ffk9tXLyVj1Wrgk2/69c/fv+R72YcOHXLjl86dTsbmDyy6Y2fa0268FfUCcHqoN9v+875y9bIbbwfHrha+tPz1Jr7PTjec3m8odjO7Z4fNj0bjhBCThZbLCpEJErsQmSCxC5EJErsQmSCxC5EJI09x9VrdFhU/3dKzJEg/kdS3K3rAaV0cWSVvvPGGGz937px/6C2/7bJ3TqPWwgX9eNNJEwWA1VXHSgWwVqRtx9lg+fR0YFmiGrVsTtulhdOCG4jTb9HyX2/NoGXz8Kw3lZIWInskdiEyQWIXIhMkdiEyQWIXIhMkdiEyQWIXIhNG3LLZXN/XIk/Y8R9Db9KfWIi1HJ8dvqd64oSf7n/hQrrlMgAc2DPvxisVJz03KM8drU9oBX7x2lW/pPJGLT03r9QzAGwFbZERpJla4KW7u25HCzP882bBeWt7f5fo2F68TClpIcTvBhK7EJkgsQuRCRK7EJkgsQuRCRK7EJkgsQuRCSPOZzdsbaW99HrF99mt6cRrvjcZ5wgH4x0Dsx20Dr74ru+ju312AcwFed3T0+mSy64HD6AatHQu/KcGFMEKhkr6esKgfLdFNQqCa1Xb8bpbzaZ/7OB5NVrlxsPScw/bi7tx5bMLkT0SuxCZILELkQkSuxCZILELkQkSuxCZILELkQkjrxvfdPxNBvnLrKZ9Uwa+aZW+p9uOfHbHNrWg9nqt6h+7Mjvjxudn/NbFnpfe8hKcEeezFxX/JVKp+fFqLV3bPRpbRG2P3aj/3D0PHgDa5v9N20HOeSV4vbVK1GYAnJoQzqjwyk7yepK/IPkayVdJfr27fS/JZ0ie7H5fiPYlhBgfvXyMbwL4ppndBODPAHyN5E0AHgDwrJndCODZ7u9CiAklFLuZrZjZi92fVwG8DuAwgDsBHOs+7BiAu4Y0RyHEAPhYN+hIfgrAZwD8CsCima10Q+8AWEyMOUJymeTyxoZfc0wIMTx6FjvJeQA/BfANM7u8PWadOwo73hsws6NmtmRmS9PTU6UmK4Ton57ETrKGjtB/ZGY/624+S/JgN34QgN+KVAgxVkLrjR1v5lEAr5vZt7eFngJwL4CHu9+fjPZlZmg2HfssyKcsao69Ftl2TvteAEDFtzt898yf98yMb63BgrkH9phn1UQWUzOwkFhEqcNBGqpjC0bZs8Gu0Q7MN88es8AaY5CiGl0lo/Pu7j2y3txS0ulYLz77ZwF8BcArJI93tz2Ijsh/QvI+AG8DuLuHfQkhxkQodjP7JdJvRJ8b7HSEEMNCy2WFyASJXYhMkNiFyASJXYhMkNiFyIQRt2wOyvsGLZubDcePLsr57HFLZ+cRhT/v2aBcc9lVxF6Ka7UapKgGpabbTsljIC73DKdtMoP02VZgtAdZqH67aUY+eBAPFgmEawC8eLD2IU6B3Rld2YXIBIldiEyQ2IXIBIldiEyQ2IXIBIldiEyQ2IXIhJGXkva8UQtypz3b1G3nDKAdtIMOWw97fnHgi9ar6XLKANCk/57rdD0GAExNpSsA1eu+x19zxgLApl+hG0Xw3Co15/jOOe2FVtAq21u3YcHKiuBPgnawBqAddWwuUUraHBF5Y3VlFyITJHYhMkFiFyITJHYhMkFiFyITJHYhMkFiFyITRuqzt1pNvPfee8n4noW97viNK1eSsWt2+37vVOCrXnX2DQC12XTb5Nlp/z2zCHKjZ2d8rzvK82+1nTbY9Pddqfo+/FzQLnpzyzfid+3enYxZYGavbzTceBG0wi4cHz9YdRF63QxeT8GSEbc0fJSu7oW9WenKLkQmSOxCZILELkQmSOxCZILELkQmSOxCZILELkQm9NKf/XoAPwSwiI7Fd9TMvkvyIQB/A+B896EPmtnT3r5azRbeu/R+Mj41PefOpaikvfTNdd+TrVX8OFq+F26tdF36jTXfa25u+YXhGRirlcCP9vq3R/XLIw+/qPjnJdy/6ydHtdn9uTFIGjfnWhYdux11jy98jz/MSXdj/vPuNxe+l0U1TQDfNLMXSe4C8ALJZ7qx75jZP/SwDyHEmOmlP/sKgJXuz6skXwdweNgTE0IMlo/1PzvJTwH4DIBfdTfdT/Jlko+RXEiMOUJymeTyVjOocSSEGBo9i53kPICfAviGmV0G8D0AnwZwMzpX/kd2GmdmR81sycyWakHfMSHE8OhJ7CRr6Aj9R2b2MwAws7Nm1rLOnY7vA7hleNMUQpQlFDs7t3ofBfC6mX172/aD2x72JQAnBj89IcSg6OVz9WcBfAXAKySPd7c9COAekjej4yKcAvDVaEfNZgvnz19Ixnft3uOOr0851tzamjs2ak08XfVtnNZWOn5547I7dmvTt/3q8O9lVKpBqWnnuXktsgGg0fTjlcB6Q9DSuenYZ0U7sK8Y2ILBLSCv3LNFpaDD62Bgj8U9wNNjg17U/ZaS7uVu/C+xc5qs66kLISYLraATIhMkdiEyQWIXIhMkdiEyQWIXIhMkdiEyYcSlpFu4fDntSbe2Ak+3lvYQW410CmovcQQllVuttKm7dmXVHdto+Cmudb8KtlsSOYoHNjtaW8F5qfulqEHfK295hnMwOUZppNESAOcB0Xmx4AFRS2Yv7Rjw/fB2mOKqls1CCAeJXYhMkNiFyASJXYhMkNiFyASJXYhMkNiFyARGJW8HejDyPIC3t226FsC7I5vAx2NS5zap8wI0t34Z5Nw+aWb7dwqMVOwfOTi5bGZLY5uAw6TObVLnBWhu/TKqueljvBCZILELkQnjFvvRMR/fY1LnNqnzAjS3fhnJ3Mb6P7sQYnSM+8ouhBgRErsQmTAWsZO8neR/kXyT5APjmEMKkqdIvkLyOMnlMc/lMZLnSJ7Ytm0vyWdInux+37HH3pjm9hDJM91zd5zkHWOa2/Ukf0HyNZKvkvx6d/tYz50zr5Gct5H/z06yAuC/AXwewGkAzwO4x8xeG+lEEpA8BWDJzMa+AIPknwO4AuCHZvbH3W1/D+CimT3cfaNcMLO/nZC5PQTgyrjbeHe7FR3c3mYcwF0A/hpjPHfOvO7GCM7bOK7stwB408zeMrMGgB8DuHMM85h4zOw5ABc/tPlOAMe6Px9D58UychJzmwjMbMXMXuz+vArggzbjYz13zrxGwjjEfhjAr7f9fhqT1e/dAPyc5Askj4x7MjuwaGYr3Z/fAbA4zsnsQNjGe5R8qM34xJy7ftqfl0U36D7KrWb2pwC+COBr3Y+rE4l1/gebJO+0pzbeo2KHNuO/YZznrt/252UZh9jPALh+2++f6G6bCMzsTPf7OQBPYPJaUZ/9oINu9/u5Mc/nN0xSG++d2oxjAs7dONufj0PszwO4keQNJOsAvgzgqTHM4yOQnOveOAHJOQBfwOS1on4KwL3dn+8F8OQY5/JbTEob71SbcYz53I29/bmZjfwLwB3o3JH/HwB/N445JOb1+wD+s/v16rjnBuBxdD7WbaFzb+M+APsAPAvgJID/ALB3gub2LwBeAfAyOsI6OKa53YrOR/SXARzvft0x7nPnzGsk503LZYXIBN2gEyITJHYhMkFiFyITJHYhMkFiFyITJHYhMkFiFyIT/h8TqrtD9Mgs2wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0])\n",
    "print('라벨: ', y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2166f6d",
   "metadata": {},
   "source": [
    "# 딥러닝 네트워크 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d8d91da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model에 추가된 Layer 개수:  7\n"
     ]
    }
   ],
   "source": [
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "print('Model에 추가된 Layer 개수: ', len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5ec1dff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 26, 26, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 32)                25632     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 30,819\n",
      "Trainable params: 30,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d1b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a3d1dc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Reshape - x_train_norm shape: (900, 28, 28, 3)\n",
      "After Reshape - x_train_reshaped shape: (900, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Before Reshape - x_train_norm shape: {}\".format(x_train_norm.shape))\n",
    "#print(\"Before Reshape - x_test_norm shape: {}\".format(x_test_norm.shape))\n",
    "\n",
    "x_train_reshaped=x_train_norm.reshape( -1, 28, 28, 3)  # 데이터갯수에 -1을 쓰면 reshape시 자동계산됩니다.\n",
    "#x_test_reshaped=x_test_norm.reshape( -1, 28, 28, 3)\n",
    "\n",
    "print(\"After Reshape - x_train_reshaped shape: {}\".format(x_train_reshaped.shape))\n",
    "#print(\"After Reshape - x_test_reshaped shape: {}\".format(x_test_reshaped.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d20b4137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 1.0745 - accuracy: 0.4522\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.9549 - accuracy: 0.7122\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.7530 - accuracy: 0.7244\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.5456 - accuracy: 0.8300\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.3924 - accuracy: 0.8944\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.2664 - accuracy: 0.9389\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.2000 - accuracy: 0.9544\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1682 - accuracy: 0.9556\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1282 - accuracy: 0.9756\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0992 - accuracy: 0.9778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2ce9920610>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_reshaped, y_train, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3c82fbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "test_rock 이미지 resize 완료!\n",
      "\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "test_scissor 이미지 resize 완료!\n",
      "\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "test_paper 이미지 resize 완료!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test data 만들어주기\n",
    "\n",
    "# x_test, y_test를 만드는 방법은 x_train, y_train을 만드는 방법과 아주 유사합니다.\n",
    "# [[YOUR CODE]]\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test_rock\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"test_rock 이미지 resize 완료!\", end='\\n\\n')\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test_scissor\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"test_scissor 이미지 resize 완료!\", end='\\n\\n')\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test_paper\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"test_paper 이미지 resize 완료!\", end='\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9575c21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/rock_scissor_paper\n",
      "학습데이터(x_test)의 이미지 개수는 300 입니다.\n",
      "x_train shape: (300, 28, 28, 3)\n",
      "y_train shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "def load_test_data(img_path, number_of_data=300, rock_path='/test_rock/*.jpg',\\\n",
    "                   scissor_path='/test_scissor/*.jpg', paper_path='/test_paper/*.jpg'):  # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+scissor_path):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+rock_path):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+paper_path):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_test)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "print(image_dir_path)\n",
    "(x_test, y_test)=load_test_data(image_dir_path)\n",
    "x_test_norm = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_test.shape))\n",
    "print(\"y_train shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7af9f224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Reshape - x_test_norm shape: (300, 28, 28, 3)\n",
      "After Reshape - x_test_reshaped shape: (300, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Before Reshape - x_test_norm shape: {}\".format(x_test_norm.shape))\n",
    "\n",
    "x_test_reshaped=x_test_norm.reshape( -1, 28, 28, 3)\n",
    "\n",
    "print(\"After Reshape - x_test_reshaped shape: {}\".format(x_test_reshaped.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "75334f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 0.3234 - accuracy: 0.8533\n",
      "test_loss: 0.32340726256370544 \n",
      "test_accuracy: 0.8533333539962769\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85b0ae5",
   "metadata": {},
   "source": [
    "TEST ACCURACY가 높아서 적어놓습니다.<br>\n",
    "처음 사용한 학습 데이터 : https://wannabe-professional-programmer.tistory.com/43\n",
    "\n",
    "\n",
    "저는 학습 데이터를 다음과 같은 제한 조건을 주고 준비했습니다.\n",
    "\n",
    "- 배경 : 흰 배경에서 주변에 다른 물체가 보이지 않도록 최대한 노력\n",
    "- 포즈 : 손을 밑에서 위로 뻗는 형식\n",
    "- 의상 : 머리끈이나 의상이 보이지 않고 손만 보이도록 \n",
    "\n",
    "제한 조건을 준 이유는 다음과 같습니다.\n",
    "\n",
    "1. 현재 AI에서 분석하는 데이터는 일정한 환경이나 제한조건이 존재한다고 생각하기 때문입니다.\n",
    "\n",
    "의료영상 중, mri의 경우 어떤 부위를 촬영할 때 어느 사람이나 동일한 포즈로 mri를 찍습니다. 어느 병원은 컬러로 찍고, 흑백으로 찍는게 아닌, 전부 흑백(흑백이라고 표현하는게 맞는진 모르겠으나)으로 데이터가 나옵니다.\n",
    "\n",
    "의료영상 중 수술 영상의 경우, 발 수술하는데 복부 개복하지 않는것 처럼, 어느 부위를 수술할 때 개복하는 위치나, 초소형 카메라가 들어가는 위치의 범위는 대체로 유사합니다. 또한, 유사한 수술기법이기 때문에, 수술영상에서 보이는 장기나, 사용하는 수술 기구 또한 변하지 않으며 추측컨데 조도 또한 유사할 것입니다.(어둡게 수술하지않을테니까요)\n",
    "\n",
    "자율주행에서도, 블랙박스영상이나 라이다가 차 내에 고정이 되어있지 해당 센서가 모든곳에서 움직이진 않습니다. 즉, 고정된 위치에서 한정된 데이터를 읽어와 분석합니다.\n",
    "\n",
    "위와 같은 생각으로 저는, 가위바위보에도 제한조건이 필요하다고 생각했습니다. 가위바위보는 보통 서서하지, 뛰면서 하지 않습니다. 그렇기 때문에 배경은 고정일 것이므로 배경에 대한 노이즈를 최대한 줄일 흰색 배경에서 데이터를 생성하였습니다.\n",
    "\n",
    "가위바위보는 밑에서 위로 내거나 아래에서 위로내는 등, 내는 방법은 다양하지만, 결국 가위나 바위, 보를 냈을 결과에 대한 포즈는 동일합니다. 이 테스트는 영상이 아니라 사진을 분석하는 것이며, 이미 낸 상황에 데이터를 분석하는 것입니다. 즉, 손의 위치가 오른쪽에서 나오거나 왼쪽에서 나오거나 둘 중하나이며 카메라가 고정되어있을 상황에서 위쪽에서 가위바위보를 냈거나 아랫쪽에서 가위바위보를 낸 상황은 없을것입니다. 그렇기 때문에 저는 아래쪽에서 위쪽으로 낸 상황을 설정하여 학습시켰습니다.\n",
    "~~뜬금없이 왜 아래쪽에서 위쪽이냐 라고하시면, 오른쪽 왼쪽 데이터를 각각 생성하는게 귀찮았으며(죄송합니다) 컴퓨터가 가위, 바위, 보를 구별하기만 하면 됐지 내가 이상한 데이터를 준걸 모를거기 때문입니다.(컴퓨터왕무시 ㅎ_ㅎㅋ)~~\n",
    "\n",
    "마지막으로 의상의 경우는 카메라가 의상이라거나 그런 노이즈에 의해서 오판을 하는것이 아니라 사람마다 다른 손의 크기나 피부색, 카메라와의 거리, 손에 힘을 준 정도 등에 따라 다르게 판단하면 안된다라고 생각했습니다. 그래서 손의 모양과 색깔에만 집중될 수 있도록 손만 나오게 데이터를 생성하면서 다양한 각도로 데이터를 생성하였습니다.\n",
    "\n",
    "2. 첫 Test Accuracy가 너무 낮으면 그것은 학습이 된것이 아니라고 생각합니다.\n",
    "\n",
    "너무 낮은경우 딥러닝 모델의 문제가 아니라 데이터 자체에 문제가 있다고 생각합니다. 적어도 정확도가 80%는 넘어야한다고 생각하였습니다. 그러려면 더더욱 제한조건이 필요하다고 생각하였습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f740c74f",
   "metadata": {},
   "source": [
    "# 성능 개선 방향 1. 학습 데이터의 픽셀을 높여본다.\n",
    "\n",
    "- 기존의 학습데이터는 224*224 이미지 크기에서 28*28 로 줄여지므로 이미 작은 픽셀로 데이터를 생성하여 해상도를 높이면 학습 accuracy가 더 높아질것으로 예상하였습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b8b815e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150  images to be resized.\n",
      "150  images resized.\n",
      "바위 이미지 resize 완료!\n",
      "\n",
      "\n",
      "150  images to be resized.\n",
      "150  images resized.\n",
      "가위 이미지 resize 완료!\n",
      "\n",
      "\n",
      "150  images to be resized.\n",
      "150  images resized.\n",
      "보 이미지 resize 완료!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 96*96 이미지로 학습데이터 생성\n",
    "\n",
    "# 1. resize images\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/better_train_rock\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"바위 이미지 resize 완료!\\n\\n\")\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/better_train_scissor\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"가위 이미지 resize 완료!\\n\\n\")\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/better_train_paper\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"보 이미지 resize 완료!\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "184cbfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 450 입니다.\n",
      "x_better_train shape: (450, 28, 28, 3)\n",
      "0.9176470588235294 0.03137254901960784\n",
      "y_better_train shape: (450,)\n"
     ]
    }
   ],
   "source": [
    "# 2. load images\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_better_train, y_better_train)=load_data(image_dir_path, number_of_data=450,rock_path='/better_train_rock/*.jpg',\\\n",
    "                             paper_path='/better_train_paper/*.jpg', scissor_path='/better_train_scissor/*.jpg')\n",
    "x_better_train_norm = x_better_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_better_train shape: {}\".format(x_better_train.shape))\n",
    "print(np.max(x_better_train_norm), np.min(x_better_train_norm))\n",
    "print(\"y_better_train shape: {}\".format(y_better_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6c409b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Reshape - x_better_train_norm shape: (450, 28, 28, 3)\n",
      "After Reshape - x_better_train_norm shape: (450, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "# 3. normalization\n",
    "print(\"Before Reshape - x_better_train_norm shape: {}\".format(x_better_train_norm.shape))\n",
    "\n",
    "x_better_train_reshaped=x_better_train_norm.reshape( -1, 28, 28, 3)  # 데이터갯수에 -1을 쓰면 reshape시 자동계산됩니다.\n",
    "\n",
    "print(\"After Reshape - x_better_train_norm shape: {}\".format(x_better_train_reshaped.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "83d5686e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model에 추가된 Layer 개수:  7\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0881 - accuracy: 0.4689\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0368 - accuracy: 0.5511\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.9606 - accuracy: 0.7067\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.8391 - accuracy: 0.7533\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6940 - accuracy: 0.8067\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.8133\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4088 - accuracy: 0.8600\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3458 - accuracy: 0.8867\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2532 - accuracy: 0.9133\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1978 - accuracy: 0.9511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2ce9a7bc40>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. new train\n",
    "better_model=keras.models.Sequential()\n",
    "better_model.add(keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "better_model.add(keras.layers.MaxPool2D(2,2))\n",
    "better_model.add(keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
    "better_model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "better_model.add(keras.layers.Flatten())\n",
    "better_model.add(keras.layers.Dense(32, activation='relu'))\n",
    "better_model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "print('Model에 추가된 Layer 개수: ', len(better_model.layers))\n",
    "\n",
    "better_model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "better_model.fit(x_better_train_reshaped, y_better_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8c17d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "better_test_rock 이미지 resize 완료!\n",
      "\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "better_test_scissor 이미지 resize 완료!\n",
      "\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "better_test_paper 이미지 resize 완료!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 96*96 이미지로 테스트데이터 생성\n",
    "\n",
    "# resize test images\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/better_test_rock\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"better_test_rock 이미지 resize 완료!\", end='\\n\\n')\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/better_test_scissor\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"better_test_scissor 이미지 resize 완료!\", end='\\n\\n')\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/better_test_paper\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"better_test_paper 이미지 resize 완료!\", end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "33f6314b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/rock_scissor_paper\n",
      "학습데이터(x_test)의 이미지 개수는 300 입니다.\n",
      "x_better_test_norm shape: (300, 28, 28, 3)\n",
      "y_better_test shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "# 2. load test images\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "print(image_dir_path)\n",
    "(x_better_test, y_better_test)=load_test_data(image_dir_path, 300,\\\n",
    "                                              '/better_test_rock/*.jpg','/better_test_rock/*.jpg','/better_test_rock/*.jpg' )\n",
    "x_better_test_norm = x_better_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_better_test_norm shape: {}\".format(x_better_test_norm.shape))\n",
    "print(\"y_better_test shape: {}\".format(y_better_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c4d1692f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Reshape - x_better_test_norm shape: (300, 28, 28, 3)\n",
      "After Reshape - x_better_test_reshaped shape: (300, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "# 3. normalization\n",
    "\n",
    "print(\"Before Reshape - x_better_test_norm shape: {}\".format(x_better_test_norm.shape))\n",
    "x_better_test_reshaped=x_better_test_norm.reshape( -1, 28, 28, 3)\n",
    "print(\"After Reshape - x_better_test_reshaped shape: {}\".format(x_better_test_reshaped.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5a6ed0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 2.7094 - accuracy: 0.3333\n",
      "better_test_loss: 2.709434986114502 \n",
      "better_test_accuracy: 0.3333333432674408\n"
     ]
    }
   ],
   "source": [
    "# 4. test\n",
    "\n",
    "better_test_loss, better_test_accuracy = better_model.evaluate(x_better_test_reshaped,y_better_test, verbose=2)\n",
    "print(\"better_test_loss: {} \".format(better_test_loss))\n",
    "print(\"better_test_accuracy: {}\".format(better_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b3eb4d",
   "metadata": {},
   "source": [
    "처참하게 망했네요! ^_^/  ~~(인생.....)~~\n",
    "\n",
    "기존의 데이터는 학습데이터가 300씩 900장이고, 테스트데이터는 100씩 300장이었다면,<br>\n",
    "현재 데이터는 학습데이터가 150씩 450장이고, 테스트 데이터는 100씩 300장이었습니다.\n",
    "\n",
    "혹시 학습이 부족한가 싶어, 똑같은 조건으로 다시 시도해보았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2ae51d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300  images to be resized.\n",
      "300  images resized.\n",
      "바위 이미지 resize 완료!\n",
      "\n",
      "\n",
      "300  images to be resized.\n",
      "300  images resized.\n",
      "가위 이미지 resize 완료!\n",
      "\n",
      "\n",
      "300  images to be resized.\n",
      "300  images resized.\n",
      "보 이미지 resize 완료!\n",
      "\n",
      "\n",
      "학습데이터(x_train)의 이미지 개수는 900 입니다.\n",
      "x_bb_train shape: (900, 28, 28, 3)\n",
      "0.8431372549019608 0.0196078431372549\n",
      "y_bb_train shape: (900,)\n",
      "Before Reshape - x_bb_train_norm shape: (900, 28, 28, 3)\n",
      "After Reshape - x_bb_train_norm shape: (900, 28, 28, 3)\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.2982 - accuracy: 0.9089\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1796 - accuracy: 0.9711\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.1213 - accuracy: 0.9878\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0738 - accuracy: 0.9967\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9978\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0345 - accuracy: 0.9978\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0360 - accuracy: 0.9978\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.9989\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0147 - accuracy: 0.9989\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0150 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2c2c03b220>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 96*96 이미지로 학습데이터 생성\n",
    "\n",
    "# 1. resize images\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/bb_train_rock\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"바위 이미지 resize 완료!\\n\\n\")\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/bb_train_scissor\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"가위 이미지 resize 완료!\\n\\n\")\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/bb_train_paper\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"보 이미지 resize 완료!\\n\\n\")\n",
    "\n",
    "# 2. load images\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_bb_train, y_bb_train)=load_data(image_dir_path, number_of_data=900,rock_path='/bb_train_rock/*.jpg',\\\n",
    "                             paper_path='/bb_train_paper/*.jpg', scissor_path='/bb_train_scissor/*.jpg')\n",
    "x_bb_train_norm = x_bb_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_bb_train shape: {}\".format(x_bb_train.shape))\n",
    "print(np.max(x_bb_train_norm), np.min(x_bb_train_norm))\n",
    "print(\"y_bb_train shape: {}\".format(y_bb_train.shape))\n",
    "\n",
    "# 3. normalization\n",
    "print(\"Before Reshape - x_bb_train_norm shape: {}\".format(x_bb_train_norm.shape))\n",
    "\n",
    "x_bb_train_reshaped=x_bb_train_norm.reshape( -1, 28, 28, 3)  # 데이터갯수에 -1을 쓰면 reshape시 자동계산됩니다.\n",
    "\n",
    "print(\"After Reshape - x_bb_train_norm shape: {}\".format(x_bb_train_reshaped.shape))\n",
    "\n",
    "# 4. new train\n",
    "better_model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "better_model.fit(x_bb_train_reshaped, y_bb_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "22abe701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "better_test_rock 이미지 resize 완료!\n",
      "\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "better_test_scissor 이미지 resize 완료!\n",
      "\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "better_test_paper 이미지 resize 완료!\n",
      "\n",
      "/aiffel/aiffel/rock_scissor_paper\n",
      "학습데이터(x_test)의 이미지 개수는 300 입니다.\n",
      "x_better_test_norm shape: (300, 28, 28, 3)\n",
      "y_better_test shape: (300,)\n",
      "Before Reshape - x_better_test_norm shape: (300, 28, 28, 3)\n",
      "After Reshape - x_better_test_reshaped shape: (300, 28, 28, 3)\n",
      "10/10 - 0s - loss: 4.3265 - accuracy: 0.3333\n",
      "better_test_loss: 4.326506614685059 \n",
      "better_test_accuracy: 0.3333333432674408\n"
     ]
    }
   ],
   "source": [
    "# 96*96 이미지로 테스트데이터 생성\n",
    "\n",
    "# resize test images\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/better_test_rock\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"better_test_rock 이미지 resize 완료!\", end='\\n\\n')\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/better_test_scissor\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"better_test_scissor 이미지 resize 완료!\", end='\\n\\n')\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/better_test_paper\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"better_test_paper 이미지 resize 완료!\", end='\\n\\n')\n",
    "\n",
    "# 2. load test images\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "print(image_dir_path)\n",
    "(x_better_test, y_better_test)=load_test_data(image_dir_path, 300,\\\n",
    "                                              '/better_test_rock/*.jpg','/better_test_rock/*.jpg','/better_test_rock/*.jpg' )\n",
    "x_better_test_norm = x_better_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_better_test_norm shape: {}\".format(x_better_test_norm.shape))\n",
    "print(\"y_better_test shape: {}\".format(y_better_test.shape))\n",
    "\n",
    "# 3. normalization\n",
    "\n",
    "print(\"Before Reshape - x_better_test_norm shape: {}\".format(x_better_test_norm.shape))\n",
    "x_better_test_reshaped=x_better_test_norm.reshape( -1, 28, 28, 3)\n",
    "print(\"After Reshape - x_better_test_reshaped shape: {}\".format(x_better_test_reshaped.shape))\n",
    "\n",
    "# 4. test\n",
    "\n",
    "better_test_loss, better_test_accuracy = better_model.evaluate(x_better_test_reshaped,y_better_test, verbose=2)\n",
    "print(\"better_test_loss: {} \".format(better_test_loss))\n",
    "print(\"better_test_accuracy: {}\".format(better_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf36c1fe",
   "metadata": {},
   "source": [
    "# 결론\n",
    "\n",
    "처음 생각으론 사진의 축소를 정도가 덜하면 해상도가 좋아지니, 모델의 성능이 좋아질거라 예상했다.\n",
    "하지만 결과는 처참하게 성능이 떨어졌다.~~망했다.~~\n",
    "\n",
    "결과가 나온 것을 찬찬히 살펴보니, 이미 기존에 train할때 epoch당 accuracy가 너무 높았다. 오버피팅이 되었다 라고 판단하는게 맞는진 모르겠으나, 오버피팅이 되었다라고,, 판단하는게 맞는것 같다. 맞는진 후에, 아이펠 사람들에게 물어볼 예정.\n",
    "\n",
    "학습 데이터와 테스트 데이터를 찬찬히 살펴보니, 움직이면서 픽셀의 유동화? 잔상이 많았다. 이미지의 축소가 조금만 이뤄지면서 이 잔상까지 학습이 된게 실패요인 이라고 생각한다. CNN에서 모델층을 지나가면서 이미지가 작아지는 것이 이미지의 지역적 특징 요소를 학습할 수 있는 효과가 있다고 배웠다. 큰 이미지를 축소하면, 잔상과 같은 노이즈들이 무시되어 이미지의 형태가 학습이 되지만, 아마 작은 이미지는 축소되는 양이 한정적이라, 잔상에 대한 학습도 진행되어 성능이 망했다고 생각한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f7950b",
   "metadata": {},
   "source": [
    "# 성능 개선 방향 2. 하이퍼 파라미터 변경 / model 수정\n",
    "\n",
    "- 성능 개선 방향 1에서, 96\\*96 이미지는 처참히 망했기 때문에, 224\\*224 이미지 데이터로 학습과 테스트를 진행하려한다.\n",
    "이 아이디어에서는 모델의 구조를 변경해보거나, 하이퍼파라미터를 변경하여 성능개선을 해보려 한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "75b2e1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 900 입니다.\n",
      "x_new_train shape: (900, 28, 28, 3)\n",
      "0.9647058823529412 0.0\n",
      "y_new_train shape: (900,)\n",
      "Before Reshape - x_new_train_norm shape: (900, 28, 28, 3)\n",
      "After Reshape - x_new_train_norm shape: (900, 28, 28, 3)\n",
      "Model에 추가된 Layer 개수:  11\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 1.1063 - accuracy: 0.3167\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 1.0910 - accuracy: 0.3789\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 1.0768 - accuracy: 0.4556\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 1.0277 - accuracy: 0.4789\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.9368 - accuracy: 0.5656\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.7671 - accuracy: 0.7122\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.5920 - accuracy: 0.7756\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.4892 - accuracy: 0.8244\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.3731 - accuracy: 0.8800\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.2852 - accuracy: 0.9100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2bc816e6a0>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 224*224 이미지로 학습\n",
    "\n",
    "# 1. resize images 이미 되어있으므로 skip\n",
    "\n",
    "# 2. load images\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_new_train, y_new_train)=load_data(image_dir_path)\n",
    "x_new_train_norm = x_new_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_new_train shape: {}\".format(x_new_train.shape))\n",
    "print(np.max(x_new_train_norm), np.min(x_new_train_norm))\n",
    "print(\"y_new_train shape: {}\".format(y_new_train.shape))\n",
    "\n",
    "# 3. normalization\n",
    "print(\"Before Reshape - x_new_train_norm shape: {}\".format(x_new_train_norm.shape))\n",
    "\n",
    "x_new_train_reshaped=x_new_train_norm.reshape( -1, 28, 28, 3)  # 데이터갯수에 -1을 쓰면 reshape시 자동계산됩니다.\n",
    "\n",
    "print(\"After Reshape - x_new_train_norm shape: {}\".format(x_new_train_reshaped.shape))\n",
    "\n",
    "# 4. new train\n",
    "new_model=keras.models.Sequential()\n",
    "new_model.add(keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "new_model.add(keras.layers.MaxPool2D(2,2))\n",
    "new_model.add(tf.keras.layers.Dropout(0.3))\n",
    "new_model.add(keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
    "new_model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "new_model.add(tf.keras.layers.Dropout(0.3))\n",
    "new_model.add(keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "new_model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "new_model.add(keras.layers.Flatten())\n",
    "new_model.add(keras.layers.Dense(32, activation='relu'))\n",
    "new_model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "print('Model에 추가된 Layer 개수: ', len(new_model.layers))\n",
    "\n",
    "new_model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "new_model.fit(x_new_train_reshaped, y_new_train, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "989e5529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_test)의 이미지 개수는 300 입니다.\n",
      "x_new_test_norm shape: (300, 28, 28, 3)\n",
      "y_new_test shape: (300,)\n",
      "\n",
      "\n",
      "Before Reshape - x_new_test_norm shape: (300, 28, 28, 3)\n",
      "After Reshape - x_new_test_reshaped shape: (300, 28, 28, 3)\n",
      "\n",
      "\n",
      "10/10 - 0s - loss: 0.2004 - accuracy: 0.9867\n",
      "new_test_loss: 0.20038697123527527 \n",
      "new_test_accuracy: 0.9866666793823242\n"
     ]
    }
   ],
   "source": [
    "# 224*224 이미지로 테스트\n",
    "\n",
    "# resize test images 이미 되어있으므로 skip\n",
    "\n",
    "# 2. load test images\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_new_test, y_new_test)=load_test_data(image_dir_path)\n",
    "x_new_test_norm = x_new_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_new_test_norm shape: {}\".format(x_new_test_norm.shape))\n",
    "print(\"y_new_test shape: {}\\n\\n\".format(y_new_test.shape))\n",
    "\n",
    "# 3. normalization\n",
    "\n",
    "print(\"Before Reshape - x_new_test_norm shape: {}\".format(x_new_test_norm.shape))\n",
    "x_new_test_reshaped=x_new_test_norm.reshape( -1, 28, 28, 3)\n",
    "print(\"After Reshape - x_new_test_reshaped shape: {}\\n\\n\".format(x_new_test_reshaped.shape))\n",
    "\n",
    "# 4. test\n",
    "\n",
    "new_test_loss, new_test_accuracy = new_model.evaluate(x_new_test_reshaped,y_new_test, verbose=2)\n",
    "print(\"new_test_loss: {} \".format(new_test_loss))\n",
    "print(\"new_test_accuracy: {}\".format(new_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea6caa",
   "metadata": {},
   "source": [
    "# 결론\n",
    "\n",
    "예전에 개인적으로 resnet 논문을 읽어본 적이 있었고~~이해했다곤 안했습니다.~~ dropout 기법이 매우 인상적이었습니다. 사람조차, 모든걸 기억하지 못하고 적절히 망각하는데, ai에도 적용하면 좋을 것같아, model사이에 dropout 층을 추가하였고, 혹시 최적화가 덜될까봐 conv2D층을 더 추가해주었습니다.\n",
    "\n",
    "그 결과 test_accuracy가 85에서 98 정확도까지 올랐습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abbc42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b84c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48843650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
