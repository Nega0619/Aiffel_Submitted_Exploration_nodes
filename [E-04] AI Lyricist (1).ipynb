{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e35cfbf3",
   "metadata": {},
   "source": [
    "오늘은, 노래가사를 input data로 이용하여, 작사를 하는 AI인공지능을 만들어 보았습니다.\n",
    "\n",
    "제가 사용한 노래는 Adele, Beatles, Bieber, Britney-spears, Bruno Mars, Disney,michael-jackson, rihanna, paul-smith의 노래입니다.\n",
    "\n",
    "제공받은 노래의 종류는 더 많지만 그것을 전부 사용하지 않은 이유는 다음과 같습니다.\n",
    "\n",
    "1. 전체 노래를 학습시키기에는 양이 너무 많다. 학습시킬 데이터 양이 많으면 시간이 오래걸릴거라 생각하여 선별했습니다.\n",
    "2. 전처리를 위해서는 데이터가 어떤 형식인지 대강이라도 먼저 살펴보아야 하는데, 그 모든 노래를 살펴볼수 없었다.\n",
    "    - 너무 반복되는 문장을 삭제하기 위함입니다.\n",
    "3. 제가 아는 가수 위주로 골랐습니다.\n",
    "    - 이 이유는, 노래의 분위기를 맞춰주려고 한 것입니다. 예를 들어 신문기사를 쓰는 AI를 작성하려는데, 아동의 동요 데이터도 같이 학습 시킨다면, 이는 노이즈 데이터라고 생각합니다.\n",
    "    - 이와 같은 이유로, eminem가수를 저는 알지만, disney노래와는 잘 맞지 않는 분위기입니다. 그래서 제외시켰습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d1da1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 34896\n",
      "Examples:\n",
      " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/Aiffel_Exploration_nodes/lyricist/mydata/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21e4f60",
   "metadata": {},
   "source": [
    "# 1. 데이터 전처리하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df0e3bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#        strip문자의 특성\n",
    "#     2. 특수문자는 삭제해줍니다.\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,'¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\"\", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?\\'|.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd8e565",
   "metadata": {},
   "source": [
    "삭제 리스트 \n",
    "데이터 셋을 훑어보고 5회이상 같은 가사가 반복되면 삭제시켰습니다.\n",
    "각종 특수문자가 포함된 문장은 삭제시켰습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7a9e6ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<start> looking for some education <end>',\n",
       " '<start> made my way into the night <end>',\n",
       " '<start> all that bullshit conversation <end>',\n",
       " \"<start> baby can't you read the signs i won't bore you with the details baby <end>\",\n",
       " \"<start> i don't even wanna waste your time <end>\",\n",
       " \"<start> let's just say that maybe <end>\",\n",
       " '<start> you could help me ease my mind <end>',\n",
       " \"<start> i ain't mr right but if you're looking for fast love <end>\",\n",
       " \"<start> if that's love in your eyes <end>\",\n",
       " \"<start> it's more than enough <end>\",\n",
       " '<start> had some bad love <end>',\n",
       " \"<start> so fast love is all that i've got on my mind ooh ooh <end>\",\n",
       " '<start> ooh ooh looking for some affirmation <end>',\n",
       " '<start> made my way into the sun <end>',\n",
       " '<start> my friends got their ladies <end>',\n",
       " \"<start> and they're all having babies <end>\",\n",
       " \"<start> i just wanna have some fun i won't bore you with the details baby <end>\",\n",
       " \"<start> i don't even wanna waste your time <end>\",\n",
       " \"<start> let's just say that maybe <end>\",\n",
       " '<start> you could help me ease my mind <end>',\n",
       " \"<start> i ain't mr right but if you're looking for fast love <end>\",\n",
       " \"<start> if that's love in your eyes <end>\",\n",
       " \"<start> it's more than enough <end>\",\n",
       " \"<start> i've had some bad love <end>\",\n",
       " \"<start> so fast love is all that i've got on my mind ooh ooh <end>\",\n",
       " '<start> baby baby <end>',\n",
       " '<start> ooh ooh <end>',\n",
       " '<start> baby baby <end>',\n",
       " '<start> ooh ooh <end>',\n",
       " '<start> baby baby <end>']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모을겁니다\n",
    "corpus = []\n",
    "\n",
    "\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence.find(\":\")==1: continue\n",
    "    if sentence.find(\"]\")==1: continue\n",
    "    if sentence.find(\")\")==1: continue\n",
    "    if sentence.find(\"*\")==1: continue\n",
    "    #if sentence.find(\"-\")==1: continue\n",
    "    \"\"\"\n",
    "    if sentence.find(\"ooo\")==1: continue\n",
    "    if sentence.find(\"We are the new America\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"This is the new America\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"American, American oxygen American oxygen\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"Baby I love you\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"Why You Wanna Trip On Me\")==1: \n",
    "        continue    \n",
    "    if sentence.find(\"Baby don't make me\")==1: \n",
    "        continue    \n",
    "    if sentence.find(\"He really thought he really had\")==1: \n",
    "        continue    \n",
    "    if sentence.find(\"They thought they really had control of me\")==1: \n",
    "        continue            \n",
    "    if sentence.find(\"Dom Sheldon is a cold man\")==1: \n",
    "        continue    \n",
    "    if sentence.find(\"Where Did She go\")==1: \n",
    "        continue            \n",
    "    if sentence.find(\"Im Looking For The Girl\")==1: \n",
    "        continue    \n",
    "    if sentence.find(\"Gimme\")==1: \n",
    "        continue    \n",
    "    if sentence.find(\"Not alone not alone not alone\")==1: \n",
    "        continue            \n",
    "    if sentence.find(\"You're fillin me up\")==1: \n",
    "        continue    \n",
    "    if sentence.find(\"baby, baby,\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"Get naked\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"oh, oh\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"What do you mean?\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"That should be me\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"Madly in love\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"I'll show you\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"I smile\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"Nothing's gonna change my world\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"Nah nah nah nah nah nah, nah nah nah, hey Jude\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"Sun, sun, sun, here it comes\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"I want to hold your hand\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"Come together, yeah\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"All together now\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"One, two, three, four, five, six, seven\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"All good children go to heaven\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"We're on our way home\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"Rumor has it, ooh\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"Say it ain't so, say it ain't so\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"We will stand tall\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"When it crumbles\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"I miss you, I miss you\")==1: \n",
    "        continue\n",
    "    if sentence.find(\"Why do you love me\")==1: \n",
    "        continue\n",
    "    \"\"\"                 \n",
    "        \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "\n",
    "print(len(corpus))\n",
    "corpus[:30]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a82ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "672045bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2 226  25 ...   0   0   0]\n",
      " [  2 283  12 ...   0   0   0]\n",
      " [  2  22  15 ...   0   0   0]\n",
      " ...\n",
      " [  2  17  17 ...   0   0   0]\n",
      " [  2  18  22 ...   0   0   0]\n",
      " [  2  17  17 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f1699462e20>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n",
    "# 더 잘 알기 위해 아래 문서들을 참고하면 좋습니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "def tokenize(corpus):\n",
    "    # 7000단어를 기억할 수 있는 tokenizer를 만들겁니다\n",
    "    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n",
    "    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=10000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "521f6bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : you\n",
      "5 : i\n",
      "6 : the\n",
      "7 : me\n",
      "8 : to\n",
      "9 : and\n",
      "10 : it\n",
      "11 : a\n",
      "12 : my\n",
      "13 : in\n",
      "14 : love\n",
      "15 : that\n",
      "16 : your\n",
      "17 : oh\n",
      "18 : i'm\n",
      "19 : on\n",
      "20 : be\n",
      "21 : don't\n",
      "22 : all\n",
      "23 : of\n",
      "24 : is\n",
      "25 : for\n",
      "26 : like\n",
      "27 : so\n",
      "28 : know\n",
      "29 : baby\n",
      "30 : yeah\n",
      "31 : just\n",
      "32 : we\n",
      "33 : but\n",
      "34 : no\n",
      "35 : what\n",
      "36 : do\n",
      "37 : with\n",
      "38 : it's\n",
      "39 : you're\n",
      "40 : got\n",
      "41 : up\n",
      "42 : when\n",
      "43 : can\n",
      "44 : get\n",
      "45 : now\n",
      "46 : if\n",
      "47 : one\n",
      "48 : girl\n",
      "49 : this\n",
      "50 : go\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 50: break\n",
    "#어떻게 구축되었는지.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ebe82a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33204\n",
      "[  2  38  87 202 203   3   0   0   0   0   0   0   0   0]\n",
      "[ 38  87 202 203   3   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "print(len(src_input))\n",
    "print(src_input[9])\n",
    "print(tgt_input[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3b2225a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (23242, 14)\n",
      "Target Train: (23242, 14)\n"
     ]
    }
   ],
   "source": [
    "# 평가 데이터셋 분리\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                    tgt_input, \n",
    "                                                    test_size=0.3)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "803189d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33204 129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "print(BUFFER_SIZE, steps_per_epoch)\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
    "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a6ac957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.drop1 = tf.keras.layers.Dropout(0.3)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.drop2 = tf.keras.layers.Dropout(0.3)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.drop1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.drop2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aa2b66d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 10001), dtype=float32, numpy=\n",
       "array([[[-1.34137459e-04, -2.55208317e-04, -3.03804482e-05, ...,\n",
       "          2.88460084e-04,  2.41612273e-04, -1.80752322e-05],\n",
       "        [-2.90620519e-04, -3.90627771e-04, -1.74332672e-04, ...,\n",
       "          3.33910139e-04,  1.36578383e-04,  1.95199376e-04],\n",
       "        [-4.42413148e-04, -6.70602429e-04, -2.33741201e-04, ...,\n",
       "          6.02601736e-04,  2.63920781e-04,  3.12030897e-04],\n",
       "        ...,\n",
       "        [ 5.72616991e-05,  2.45018862e-04,  1.15367572e-03, ...,\n",
       "          1.82833034e-03,  7.26334460e-04, -1.89103733e-03],\n",
       "        [-7.65394288e-05,  2.10251485e-04,  1.25895196e-03, ...,\n",
       "          1.66818290e-03,  3.30021780e-04, -2.05096230e-03],\n",
       "        [-2.44880765e-04,  1.97050816e-04,  1.37892750e-03, ...,\n",
       "          1.48230547e-03, -1.09067318e-04, -2.21078936e-03]],\n",
       "\n",
       "       [[ 2.23472976e-04,  2.00384711e-05,  4.39451454e-04, ...,\n",
       "         -8.95697740e-05,  2.85182963e-04,  1.73897090e-04],\n",
       "        [ 1.71744832e-04,  6.01300744e-05,  6.38239610e-04, ...,\n",
       "         -1.50106964e-04,  2.25269992e-04,  4.71224077e-04],\n",
       "        [-3.96593823e-05,  2.63687543e-04,  5.73367113e-04, ...,\n",
       "          2.90143780e-05,  8.95460325e-05,  6.56164368e-04],\n",
       "        ...,\n",
       "        [ 1.36483961e-03, -1.44276139e-03,  1.27407745e-03, ...,\n",
       "          1.33370981e-03, -2.93964613e-03, -6.98910444e-05],\n",
       "        [ 1.35212892e-03, -1.84039865e-03,  1.41984841e-03, ...,\n",
       "          1.40838127e-03, -3.33282002e-03, -1.86744117e-04],\n",
       "        [ 1.29944575e-03, -2.20582495e-03,  1.55128527e-03, ...,\n",
       "          1.47303648e-03, -3.63943959e-03, -2.77058251e-04]],\n",
       "\n",
       "       [[ 2.23472976e-04,  2.00384711e-05,  4.39451454e-04, ...,\n",
       "         -8.95697740e-05,  2.85182963e-04,  1.73897090e-04],\n",
       "        [ 2.32619917e-04, -3.29978393e-05,  5.07759512e-04, ...,\n",
       "          1.17769458e-04,  4.66259458e-04,  4.57631249e-04],\n",
       "        [ 2.26965683e-04, -6.53145325e-05,  5.17667097e-04, ...,\n",
       "          2.15592663e-04,  6.71875314e-04,  4.83289507e-04],\n",
       "        ...,\n",
       "        [ 1.94603039e-04,  2.10245649e-04,  1.94968423e-04, ...,\n",
       "          3.20224935e-04,  2.78227992e-04, -5.08585763e-05],\n",
       "        [ 4.32415982e-04,  7.07809158e-05,  3.89159948e-04, ...,\n",
       "          3.87873239e-04, -3.10985197e-04, -2.28335412e-04],\n",
       "        [ 6.42374798e-04, -1.92321284e-04,  6.27173751e-04, ...,\n",
       "          4.80453600e-04, -9.59793339e-04, -4.24007478e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.23472976e-04,  2.00384711e-05,  4.39451454e-04, ...,\n",
       "         -8.95697740e-05,  2.85182963e-04,  1.73897090e-04],\n",
       "        [ 4.67310834e-04,  1.74672416e-04,  6.81987498e-04, ...,\n",
       "          8.03881776e-05,  3.82664177e-04,  5.59439677e-08],\n",
       "        [ 5.43230330e-04,  2.01997536e-04,  7.37695897e-04, ...,\n",
       "          1.58007431e-04,  7.29487103e-04, -7.93386280e-05],\n",
       "        ...,\n",
       "        [ 4.19673335e-04, -2.06614000e-04, -2.45180621e-04, ...,\n",
       "          5.02571755e-04, -5.06979646e-04,  6.72104128e-04],\n",
       "        [ 6.84730359e-04, -3.81644531e-06,  1.32044661e-04, ...,\n",
       "          6.53894094e-04, -2.04597629e-04,  4.52944223e-04],\n",
       "        [ 9.61983751e-04,  1.16156145e-04,  1.53114906e-05, ...,\n",
       "          5.19691035e-04, -4.75596986e-04,  9.41732287e-05]],\n",
       "\n",
       "       [[ 2.23472976e-04,  2.00384711e-05,  4.39451454e-04, ...,\n",
       "         -8.95697740e-05,  2.85182963e-04,  1.73897090e-04],\n",
       "        [ 1.22583879e-04, -2.40018184e-04,  3.85725492e-04, ...,\n",
       "          3.95830393e-05,  3.62975639e-04,  2.72708538e-04],\n",
       "        [ 1.50202759e-04, -3.19666287e-04,  4.87577432e-04, ...,\n",
       "          5.29230201e-05,  4.92295367e-05,  4.41700889e-04],\n",
       "        ...,\n",
       "        [ 4.97547328e-04, -2.95307516e-04, -3.95187788e-04, ...,\n",
       "          1.61538541e-03,  2.05845499e-04,  4.07416810e-04],\n",
       "        [ 7.23735080e-04, -4.12492082e-04, -1.25902734e-04, ...,\n",
       "          1.72092055e-03, -1.76857357e-04,  2.62916408e-04],\n",
       "        [ 9.36129654e-04, -6.64102321e-04,  1.65348480e-04, ...,\n",
       "          1.81617751e-03, -6.75763527e-04,  1.03304956e-04]],\n",
       "\n",
       "       [[ 2.23472976e-04,  2.00384711e-05,  4.39451454e-04, ...,\n",
       "         -8.95697740e-05,  2.85182963e-04,  1.73897090e-04],\n",
       "        [ 4.82479227e-04, -4.07818588e-05,  5.34363440e-04, ...,\n",
       "         -1.29485115e-05,  4.79291513e-04,  2.30027625e-04],\n",
       "        [ 6.92408124e-04, -2.05962453e-04,  7.73364154e-04, ...,\n",
       "          2.14916014e-04,  3.79616162e-04,  2.28388344e-05],\n",
       "        ...,\n",
       "        [ 1.05193909e-03, -2.85631721e-03,  1.81473105e-03, ...,\n",
       "          1.16657570e-03, -2.52287835e-03, -1.17475145e-04],\n",
       "        [ 1.02436007e-03, -3.11940932e-03,  1.87386549e-03, ...,\n",
       "          1.26376457e-03, -2.88463547e-03, -2.19811205e-04],\n",
       "        [ 9.77130840e-04, -3.34855635e-03,  1.92376296e-03, ...,\n",
       "          1.35486247e-03, -3.17292754e-03, -2.96313374e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
    "# 지금은 동작 원리에 너무 빠져들지 마세요~\n",
    "for src_sample, tgt_sample in dataset.take(1):break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bee3bd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      multiple                  2560256   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  10251025  \n",
      "=================================================================\n",
      "Total params: 26,450,961\n",
      "Trainable params: 26,450,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "41e0b09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "727/727 [==============================] - 27s 34ms/step - loss: 3.2347 - val_loss: 2.9711\n",
      "Epoch 2/20\n",
      "727/727 [==============================] - 24s 33ms/step - loss: 2.8370 - val_loss: 2.7539\n",
      "Epoch 3/20\n",
      "727/727 [==============================] - 25s 34ms/step - loss: 2.6124 - val_loss: 2.5927\n",
      "Epoch 4/20\n",
      "727/727 [==============================] - 25s 34ms/step - loss: 2.4138 - val_loss: 2.4706\n",
      "Epoch 5/20\n",
      "727/727 [==============================] - 25s 35ms/step - loss: 2.2274 - val_loss: 2.3765\n",
      "Epoch 6/20\n",
      "727/727 [==============================] - 25s 35ms/step - loss: 2.0493 - val_loss: 2.3012\n",
      "Epoch 7/20\n",
      "727/727 [==============================] - 25s 35ms/step - loss: 1.8845 - val_loss: 2.2457\n",
      "Epoch 8/20\n",
      "727/727 [==============================] - 25s 35ms/step - loss: 1.7348 - val_loss: 2.1999\n",
      "Epoch 9/20\n",
      "727/727 [==============================] - 26s 35ms/step - loss: 1.6016 - val_loss: 2.1767\n",
      "Epoch 10/20\n",
      "727/727 [==============================] - 26s 35ms/step - loss: 1.4882 - val_loss: 2.1617\n",
      "Epoch 11/20\n",
      "727/727 [==============================] - 26s 35ms/step - loss: 1.3872 - val_loss: 2.1531\n",
      "Epoch 12/20\n",
      "727/727 [==============================] - 26s 35ms/step - loss: 1.3022 - val_loss: 2.1545\n",
      "Epoch 13/20\n",
      "727/727 [==============================] - 26s 36ms/step - loss: 1.2298 - val_loss: 2.1707\n",
      "Epoch 14/20\n",
      "727/727 [==============================] - 26s 36ms/step - loss: 1.1639 - val_loss: 2.1726\n",
      "Epoch 15/20\n",
      "727/727 [==============================] - 26s 36ms/step - loss: 1.1119 - val_loss: 2.1931\n",
      "Epoch 16/20\n",
      "727/727 [==============================] - 26s 36ms/step - loss: 1.0625 - val_loss: 2.2080\n",
      "Epoch 17/20\n",
      "727/727 [==============================] - 26s 36ms/step - loss: 1.0235 - val_loss: 2.2192\n",
      "Epoch 18/20\n",
      "727/727 [==============================] - 26s 36ms/step - loss: 0.9882 - val_loss: 2.2293\n",
      "Epoch 19/20\n",
      "727/727 [==============================] - 26s 36ms/step - loss: 0.9612 - val_loss: 2.2658\n",
      "Epoch 20/20\n",
      "727/727 [==============================] - 26s 36ms/step - loss: 0.9352 - val_loss: 2.2782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f16992fa790>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "#model.fit(dataset, epochs=30)\n",
    "model.fit(enc_train, dec_train, validation_data=(enc_val, dec_val),epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1c2d48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d8a8784d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> if you want the one you love <end> '"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> if\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2216c3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> l t b rap performance <end> '"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d1ca2df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> i don't know why you say goodbye i say hello wow oh hello <end> \""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1cc1b3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> how does it feel <end> '"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> how\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a3291680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> a whale of a tale or two <end> '"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2da64500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> what about us <end> '"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> what\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f27a45c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you know i love you <end> '"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "82424f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> i don't know why you say goodbye i say hello wow oh hello <end> \""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "361d57f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> i can't let her get away <end> \""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i can't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a275e961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i feel so alone <end> '"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i feel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223ed561",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "nlp에 대해서는 처음 접해봐서 이 프로젝트가 전반적으로 어떻게 돌아가는지 이해가 되지 않아 어려움이 많았다. 하지만, 다행히 MO스터디에서 도움을 받아 프로젝트를 끝마칠수는 있었다. ~~(실은, 끝내기 조차 못할뻔 했던,,,)~~\n",
    "\n",
    "아쉬웠던 점이라면, NLP모델에 대해선 미리 사전학습 해본적이 없어서, 기존 모델을 사용할 수 밖에 없었고, 현 상황에서 더 할 수 있는 것은, 데이터를 좀 더 가려서 넣어주는 것 뿐이었던 점이 아쉬웠다. 시간이 더 생긴다면, nlp 모델을 더 다양하게 써보고 싶고, 앙상블과 스태킹이라는 기법도 사용하여서 성능을 안정화 시켜보고싶다.\n",
    "\n",
    "학습시키다가 실수로 지워버렸지만, 처음 NLP의 결과물이 i로 시작하는 문장을 만들었을 때, i로 시작하는 문장을 작문했을 때, 당신이 날 얼마나 사랑하는지 느끼며 살아간다. 라는 뉘앙스로 작문이 되었었는데, ~~너무 설렜는데~~실수로 전체 셀을 지워버려서(T_T)\n",
    "결과를 날려서 너무 아쉽다.\n",
    "\n",
    "NLP실습을 하면서 제일 신기했던 것은, 데이터 전처리를 많이 해줄수록 오히려 이상한 문장을 뱉어낸다는 것이다. 처음에 전처리할 때는 같은 문장이 5번 이상 반복되면 단어를 작문할때 확률이 그 문장에 대해 비중이 커질 것같아서, 중복된 문장을 전부 찾아서 제거해주었었다. 그러면 더 다양한 문장을 생성해줄거라고 생각했는데, 오히려 더 반복되는 중복적인 문장을 뱉어내었다. 구조를 이해를 잘못한건지,, 궁금하고 공부를 더 해야겠다고 생각했다.\n",
    "또한,, 에퐄을 더 많이 돌릴수록,, 마지막 부분에 val_loss가 커졌다. 오버핏이 된상황인건가,, 어떻게 된건지 궁금하다. \n",
    "NLP는 많은 공부가 더필요할듯 싶다ㅠ_ㅠ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63be2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
